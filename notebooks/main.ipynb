{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "ASMAT_PATH=\"/Users/samir/Dev/projects/ASMAT2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import lda\n",
    "from math import log\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "#add ASMAT toolkit\n",
    "sys.path.append(ASMAT_PATH)\n",
    "sys.path.append(\"..\")\n",
    "from ASMAT import vectorizer, embeddings, features\n",
    "from ASMAT.toolkit import gensimer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths\n",
    "HOME=\"/Users/samir/Dev/projects/comment_feedback_aligner/\"\n",
    "FEEDBACK_REQUESTS_PATH = HOME+\"DATA/raw/regulations_proposed_rules_feedback.csv\"\n",
    "COMMENTS_PATH=HOME+\"DATA/raw/filtered_final_dockets_ecig.obj\"\n",
    "WORD2VEC=HOME+\"DATA/embeddings/skip_50.txt\"\n",
    "\n",
    "OUTPUT_TXT = HOME+\"DATA/processed/txt/\"\n",
    "OUTPUT_PKL = HOME+\"DATA/processed/pkl/\"\n",
    "OUTPUT_VECTORS = HOME+\"DATA/processed/vectors/\"\n",
    "\n",
    "CORPUS=OUTPUT_TXT+\"all_text.txt\"\n",
    "VOCABULARY_PATH=OUTPUT_PKL+\"vocabulary.pkl\"\n",
    "IDF_ESTIMATE_PATH=OUTPUT_PKL+\"IDF.pkl\"\n",
    "\n",
    "\n",
    "if not os.path.exists(OUTPUT_TXT):\n",
    "    os.makedirs(OUTPUT_TXT)\n",
    "if not os.path.exists(OUTPUT_PKL):\n",
    "    os.makedirs(OUTPUT_PKL)\n",
    "if not os.path.exists(OUTPUT_VECTORS):\n",
    "    os.makedirs(OUTPUT_VECTORS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Background Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_Q_LEN = 100\n",
    "\n",
    "stop_wordz = set(stopwords.words('english'))\n",
    "import string\n",
    "\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "def preprocess(d):\n",
    "    d = d.lower()\n",
    "    d = d.replace(\"\\n\",\" \").replace(\"_\",\" \")\n",
    "    #remove stop words and punctuation\n",
    "    d = \" \".join([w.translate(translator) for w in d.split() if w not in stop_wordz])\n",
    "    return d\n",
    "     \n",
    "def get_queries(path, docket_id=None):\n",
    "    queries = []   \n",
    "    df = pd.read_csv(path)\n",
    "    df.rename(columns={\"Unnamed: 0\":\"ID\"},inplace=True)\n",
    "    for _, row in df.iterrows():\n",
    "        try:        \n",
    "            fdb = row[\"feedback_asked\"]\n",
    "            fdbs = sent_tokenize(fdb)\n",
    "            docid = row[\"ID\"]\n",
    "            q = [[docid, x] for x in fdbs]\n",
    "            queries += q\n",
    "        except (AttributeError, TypeError):\n",
    "            # print(\"ERROR\")\n",
    "            continue\n",
    "    \n",
    "    if docket_id:\n",
    "        queries = [[d,d+\"#\"+str(i),q,len(q)] for i, (d,q) in enumerate(queries) if d == docket_id]\n",
    "    else:\n",
    "        queries = [[d,d+\"#\"+str(i),q,len(q)] for i, (d,q) in enumerate(queries)]\n",
    "    df = pd.DataFrame(queries,columns=[\"docketID\", \"requestID\",\"text\",\"len\"])\n",
    "    #preprocess text\n",
    "    df[\"text\"] = df[\"text\"].map(preprocess)\n",
    "    return df\n",
    "\n",
    "def get_comments(path, docket_id=None):\n",
    "    df = pd.read_json(path)\n",
    "    #filter for comments\n",
    "    df = df[df[\"documentType\"] == \"Public Submission\"] \n",
    "    #remove entries with attachments\n",
    "    df = df[df[\"attachmentCount\"] == 0]        \n",
    "    if docket_id:\n",
    "        df = df[df[\"docketId\"] == docket_id]\n",
    "    comments = []\n",
    "    for _, comment in df.iterrows():    \n",
    "        #segment comment into sentences            \n",
    "        sentences = sent_tokenize(comment[\"commentText\"])\n",
    "        # print(len(sentences))\n",
    "        c = [[ comment[\"docketId\"], comment[\"documentId\"], \\\n",
    "            comment[\"documentId\"]+\"#\"+str(i), s] \\\n",
    "            for i,s in enumerate(sentences)]\n",
    "        comments += c\n",
    "    df = pd.DataFrame(comments,columns=[\"docket_id\", \"documentId\", \"sentenceId\",\"text\"])\n",
    "    #preprocess text\n",
    "    df[\"text\"] = df[\"text\"].map(preprocess)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#read queries\n",
    "df_queries = get_queries(FEEDBACK_REQUESTS_PATH)\n",
    "print(\"queries: {}\".format(len(df_queries)))\n",
    "df_comments = get_comments(COMMENTS_PATH)\n",
    "print(\"comments: {}\".format(len(df_comments)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#extract all the text \n",
    "#%TODO: get another background corpus?\n",
    "all_text = list(df_queries[\"text\"] ) + list(df_comments[\"text\"] ) \n",
    "with open(CORPUS,\"w\") as f:\n",
    "    f.write(\"\\n\".join(all_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get vocabulary\n",
    "vocab = vectorizer.build_vocabulary(all_text, max_words=50000)\n",
    "print(\"vocabulary size: {}\".format(len(vocab)))\n",
    "#save vocabulary\n",
    "with open(VOCABULARY_PATH,\"wb\") as f:\n",
    "    pickle.dump(vocab,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inverse document frequency\n",
    "def getIDF(N, t):\n",
    "    return log(float(N)/float(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(VOCABULARY_PATH,\"rb\") as f:\n",
    "    vocab = pickle.load(f)\n",
    "with open(CORPUS,\"r\") as f:\n",
    "    all_text = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute document frequencies\n",
    "all_idxs, _ = vectorizer.docs2idx(all_text, vocab)\n",
    "ndocs = len(all_idxs)\n",
    "docfreq = Counter(str(x) for xs in all_idxs for x in set(xs))\n",
    "#inverse document frequencies\n",
    "idfs = {w: getIDF(ndocs, docfreq[w]) for w in docfreq}\n",
    "#get an IDF vector \n",
    "idfvec = np.zeros(len(idfs))\n",
    "for w, v in idfs.items(): idfvec[int(w)] = v\n",
    "with open(OUTPUT_PKL+\"/IDF.pkl\",\"wb\") as f:\n",
    "    pickle.dump(idfvec,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract word2vec embeddings\n",
    "embeddings.extract_embeddings(WORD2VEC, OUTPUT_PKL+\"word2vec.txt\", vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update word2vec embeddings \n",
    "train_seq = gensimer.Word2VecReader([CORPUS], max_sent=20000)\n",
    "w2v = gensimer.get_skipgram(dim=50,negative_samples=5)\n",
    "w2v_trained = gensimer.train_skipgram(w2v, train_seq, path_out=OUTPUT_PKL+\"word2vec_tuned.txt\",\n",
    "                                      pretrained_weights_path=OUTPUT_PKL+\"word2vec.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train topic model \n",
    "#TODO:use all data\n",
    "all_idxs, _ = vectorizer.docs2idx(all_text, vocab)\n",
    "n_topics=50\n",
    "n_iter=3\n",
    "X = features.BOW_freq(all_idxs[:100], vocab,sparse=True)\n",
    "topic_model = lda.LDA(n_topics=n_topics, n_iter=n_iter)\n",
    "X = X.astype('int32')\n",
    "topic_model.fit(X)\n",
    "#save model\n",
    "with open(OUTPUT_PKL+\"/lda.pkl\",\"wb\") as f:\n",
    "    pickle.dump([topic_model, vocab], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Get Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def docidxs(df):\n",
    "    docs = [[int(x) for x in d.split()] for d in df[\"idxs\"]]\n",
    "    return docs \n",
    "\n",
    "def get_BOW(docs, idfs):\n",
    "    X = features.BOW(docs, idfs, sparse=True)\n",
    "    return X\n",
    "\n",
    "def get_TFIDF(docs, idfs):\n",
    "    X = features.BOW_freq(docs, idfs, sparse=True)\n",
    "    X*=idfs\n",
    "    return X\n",
    "\n",
    "def get_BOE(docs, E, agg):\n",
    "    X = features.BOE(docs, E, agg)\n",
    "    return X\n",
    "\n",
    "def get_topics(docs, vocab, topic_model):\n",
    "    X = features.BOW_freq(docs, vocab, sparse=True)\n",
    "    X = X.astype('int32')\n",
    "    Xt = topic_model.transform(X)\n",
    "    return Xt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(VOCABULARY_PATH,\"rb\") as f:\n",
    "    vocab = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#read comments\n",
    "target_docket = \"FDA-2014-N-0189\"\n",
    "# target_docket = \"NPS-2017-0001\"\n",
    "df_queries = get_queries(FEEDBACK_REQUESTS_PATH,target_docket)\n",
    "df_comments = get_comments(COMMENTS_PATH,target_docket)\n",
    "qidxs, _  = vectorizer.docs2idx(df_queries[\"text\"], vocab)\n",
    "cidxs, _  = vectorizer.docs2idx(df_comments[\"text\"], vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf-idf\n",
    "with open(OUTPUT_PKL+\"/IDF.pkl\",\"rb\") as f:\n",
    "    idfvec = pickle.load(f)\n",
    "queries_tfidf = get_TFIDF(qidxs, idfvec)\n",
    "print(queries_tfidf.shape)\n",
    "comments_tfidf = get_TFIDF(cidxs, idfvec)\n",
    "print(comments_tfidf.shape)\n",
    "with open(OUTPUT_VECTORS+\"vectors_tfidf.pkl\", \"wb\") as f:\n",
    "    np.save(f,(queries_tfidf, comments_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec\n",
    "agg=\"sum\"\n",
    "E, _ = embeddings.read_embeddings(OUTPUT_PKL+\"word2vec.txt\", vocab)\n",
    "queries_boe = get_BOE(qidxs, E, agg)\n",
    "print(queries_boe.shape)\n",
    "comments_boe = get_BOE(cidxs, E, agg)\n",
    "print(comments_boe.shape)\n",
    "with open(OUTPUT_VECTORS+\"vectors_boe.pkl\", \"wb\") as f:\n",
    "    np.save(f,(queries_boe, comments_boe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec tuned\n",
    "agg=\"sum\"\n",
    "Et, _ = embeddings.read_embeddings(OUTPUT_PKL+\"word2vec_tuned.txt\", vocab)\n",
    "queries_boe = get_BOE(qidxs, Et, agg)\n",
    "print(queries_boe.shape)\n",
    "comments_boe = get_BOE(cidxs, Et, agg)\n",
    "print(comments_boe.shape)\n",
    "with open(OUTPUT_VECTORS+\"vectors_boe_tuned.pkl\", \"wb\") as f:\n",
    "    np.save(f,(queries_boe, comments_boe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#topics\n",
    "with open(OUTPUT_PKL+\"/lda.pkl\",\"rb\") as f:\n",
    "    topic_model, _ = pickle.load(f)\n",
    "queries_lda = get_topics(qidxs, vocab, topic_model)\n",
    "print(queries_lda.shape)\n",
    "comments_lda = get_topics(cidxs, vocab, topic_model)\n",
    "print(comments_lda.shape)\n",
    "with open(output_pkl+\"vectors_lda.pkl\", \"wb\") as f:\n",
    "    np.save(f,(queries_lda, comments_lda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
