{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "ASMAT_PATH=\"/Users/samir/Dev/projects/ASMAT2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import lda\n",
    "from math import log\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "#add ASMAT toolkit\n",
    "sys.path.append(ASMAT_PATH)\n",
    "sys.path.append(\"..\")\n",
    "from ASMAT import vectorizer, embeddings, features\n",
    "from ASMAT.toolkit import gensimer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths\n",
    "HOME=\"/Users/samir/Dev/projects/comment_feedback_aligner/fra/\"\n",
    "FEEDBACK_REQUESTS_PATH = HOME+\"DATA/raw/regulations_proposed_rules_feedback.csv\"\n",
    "CIGARRETES_COMMENTS_PATH=HOME+\"DATA/raw/cigarettes_regulations.obj\"\n",
    "TOBACCO_COMMENTS_PATH=HOME+\"DATA/raw/tobacco_regulations.obj\"\n",
    "WORD2VEC=HOME+\"DATA/embeddings/skip_50.txt\"\n",
    "GLOVE=HOME+\"DATA/embeddings/glove.42B.300d.txt\"\n",
    "\n",
    "OUTPUT_TXT = HOME+\"DATA/processed/txt/\"\n",
    "OUTPUT_PKL = HOME+\"DATA/processed/pkl/\"\n",
    "OUTPUT_VECTORS = HOME+\"DATA/processed/vectors/\"\n",
    "\n",
    "COMMENTS_PATH=OUTPUT_TXT+\"/all_comments.txt\"\n",
    "CORPUS=OUTPUT_TXT+\"all_text.txt\"\n",
    "VOCABULARY_PATH=OUTPUT_PKL+\"vocabulary.pkl\"\n",
    "IDF_ESTIMATE_PATH=OUTPUT_PKL+\"IDF.pkl\"\n",
    "\n",
    "\n",
    "if not os.path.exists(OUTPUT_TXT):\n",
    "    os.makedirs(OUTPUT_TXT)\n",
    "if not os.path.exists(OUTPUT_PKL):\n",
    "    os.makedirs(OUTPUT_PKL)\n",
    "if not os.path.exists(OUTPUT_VECTORS):\n",
    "    os.makedirs(OUTPUT_VECTORS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Background Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_Q_LEN = 100\n",
    "\n",
    "stop_wordz = set(stopwords.words('english'))\n",
    "import string\n",
    "\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "def preprocess(d):\n",
    "    d = d.lower()\n",
    "    d = d.replace(\"\\n\", \"\\t\")\n",
    "    #remove stop words and punctuation\n",
    "    d = \" \".join([w.translate(translator) for w in d.split() if w not in stop_wordz])\n",
    "    return d\n",
    "     \n",
    "def extract_comments(path):\n",
    "    df = pd.read_json(path)\n",
    "    #filter for comments\n",
    "    df = df[df[\"documentType\"] == \"Public Submission\"] \n",
    "    #remove empty comments\n",
    "    df.dropna(subset=['commentText'], inplace=True)\n",
    "    #remove entries with attachments\n",
    "    df = df[df[\"attachmentCount\"] == 0]        \n",
    "    #remove new lines\n",
    "    df[\"commentText\"] = df[\"commentText\"]\n",
    "    df = df[[\"docketId\", \"documentId\", \"commentText\"]]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#extract comments\n",
    "df_cigs = extract_comments(CIGARRETES_COMMENTS_PATH)\n",
    "df_tob = extract_comments(TOBACCO_COMMENTS_PATH)\n",
    "df_cigs.to_csv(COMMENTS_PATH, header=True, mode=\"w\", index=False)\n",
    "df_tob.to_csv(COMMENTS_PATH, header=False, mode=\"a\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#read queries\n",
    "df = pd.read_csv(FEEDBACK_REQUESTS_PATH)\n",
    "#extract all the text \n",
    "titles = df[\"docket_title\"].values.tolist()\n",
    "summaries = df[\"summary\"].values.tolist()\n",
    "requests = df[\"feedback_asked\"].values.tolist()\n",
    "comments = df_cigs[\"commentText\"].values.tolist() + df_tob[\"commentText\"].values.tolist()\n",
    "all_data = titles + summaries + requests + comments\n",
    "#preprocess text\n",
    "all_text = [preprocess(str(w)) for w in all_data]\n",
    "with open(CORPUS,\"w\") as f:\n",
    "    f.write(\"\\n\".join(all_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get vocabulary\n",
    "MIN_WORD_FREQ=10\n",
    "vocab = vectorizer.build_vocabulary(all_text, min_freq=MIN_WORD_FREQ)\n",
    "print(\"vocabulary size: {}\".format(len(vocab)))\n",
    "#save vocabulary\n",
    "with open(VOCABULARY_PATH,\"wb\") as f:\n",
    "    pickle.dump(vocab,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inverse document frequency\n",
    "def getIDF(N, t):\n",
    "    return log(float(N)/float(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(VOCABULARY_PATH,\"rb\") as f:\n",
    "    vocab = pickle.load(f)\n",
    "with open(CORPUS,\"r\") as f:\n",
    "    all_text = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute document frequencies\n",
    "all_idxs, _ = vectorizer.docs2idx(all_text, vocab)\n",
    "ndocs = len(all_idxs)\n",
    "docfreq = Counter(str(x) for xs in all_idxs for x in set(xs))\n",
    "#inverse document frequencies\n",
    "idfs = {w: getIDF(ndocs, docfreq[w]) for w in docfreq}\n",
    "#get an IDF vector \n",
    "idfvec = np.zeros(len(idfs))\n",
    "for w, v in idfs.items(): idfvec[int(w)] = v\n",
    "with open(OUTPUT_PKL+\"/IDF.pkl\",\"wb\") as f:\n",
    "    pickle.dump(idfvec,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract word2vec embeddings\n",
    "embeddings.extract_embeddings(GLOVE, OUTPUT_PKL+\"glove.txt\", vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update word2vec embeddings \n",
    "VECTOR_DIM=300\n",
    "NEGATIVE_SAMPLES=10\n",
    "EPOCHS=5\n",
    "train_seq = gensimer.Word2VecReader([CORPUS])\n",
    "w2v = gensimer.get_skipgram(dim=VECTOR_DIM,negative_samples=NEGATIVE_SAMPLES, min_freq=MIN_WORD_FREQ)\n",
    "w2v_trained = gensimer.train_skipgram(w2v, train_seq, epochs=EPOCHS,\n",
    "                                      path_out=OUTPUT_PKL+\"glove_tuned.txt\",\n",
    "                                      pretrained_weights_path=OUTPUT_PKL+\"glove.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train topic model \n",
    "#TODO:use all data\n",
    "all_idxs, _ = vectorizer.docs2idx(all_text, vocab)\n",
    "n_topics=100\n",
    "n_iter=100\n",
    "X = features.BOW_freq(all_idxs, vocab, sparse=True)\n",
    "X = X.astype('int32')\n",
    "topic_model = lda.LDA(n_topics=n_topics, n_iter=n_iter)\n",
    "topic_model.fit(X)\n",
    "#save model\n",
    "with open(OUTPUT_PKL+\"/lda.pkl\",\"wb\") as f:\n",
    "    pickle.dump([topic_model, vocab], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Get Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_comments(path, docket_id=None):\n",
    "    df = pd.read_json(path)\n",
    "    #filter for comments\n",
    "    df = df[df[\"documentType\"] == \"Public Submission\"] \n",
    "    #remove empty comments\n",
    "    df.dropna(subset=['commentText'], inplace=True)\n",
    "    #remove entries with attachments\n",
    "    df = df[df[\"attachmentCount\"] == 0]        \n",
    "    if docket_id:\n",
    "        df = df[df[\"docketId\"] == docket_id]\n",
    "    comments = []\n",
    "    for _, comment in df.iterrows():    \n",
    "        #segment comment into sentences            \n",
    "        txt = comment[\"commentText\"]        \n",
    "        sentences = sent_tokenize(txt)\n",
    "        # print(len(sentences))\n",
    "        c = [[ comment[\"docketId\"], comment[\"documentId\"], \\\n",
    "            comment[\"documentId\"]+\"#C\"+str(i), s] \\\n",
    "            for i,s in enumerate(sentences)]\n",
    "        comments += c\n",
    "    df = pd.DataFrame(comments,columns=[\"docketID\", \"documentID\", \"sentenceID\",\"text\"])\n",
    "    #preprocess text\n",
    "    df[\"text\"] = df[\"text\"].map(preprocess)\n",
    "    return df\n",
    "\n",
    "def get_BOW(docs, idfs):\n",
    "    X = features.BOW(docs, idfs, sparse=True)\n",
    "    return X\n",
    "\n",
    "def get_TFIDF(docs, idfs):\n",
    "    X = features.BOW_freq(docs, idfs, sparse=True)\n",
    "    X*=idfs\n",
    "    return X\n",
    "\n",
    "def get_BOE(docs, E, agg):\n",
    "    X = features.BOE(docs, E, agg)\n",
    "    return X\n",
    "\n",
    "def get_topics(docs, vocab, topic_model):\n",
    "    X = features.BOW_freq(docs, vocab, sparse=True)\n",
    "    X = X.astype('int32')\n",
    "    Xt = topic_model.transform(X)\n",
    "    return Xt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(VOCABULARY_PATH,\"rb\") as f:\n",
    "    vocab = pickle.load(f)\n",
    "#read comments\n",
    "target_docket = \"FDA-2014-N-0189\"\n",
    "# target_docket = \"NPS-2017-0001\"\n",
    "df_queries = get_queries(FEEDBACK_REQUESTS_PATH,target_docket)\n",
    "df_comments = get_comments(COMMENTS_PATH,target_docket)\n",
    "qidxs, _  = vectorizer.docs2idx(df_queries[\"text\"], vocab)\n",
    "cidxs, _  = vectorizer.docs2idx(df_comments[\"text\"], vocab)\n",
    "#save queries and comments\n",
    "df_queries.to_csv(OUTPUT_TXT+\"/queries.csv\", header=True, index=False)\n",
    "df_comments.to_csv(OUTPUT_TXT+\"/comments.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf-idf\n",
    "with open(OUTPUT_PKL+\"/IDF.pkl\",\"rb\") as f:\n",
    "    idfvec = pickle.load(f)\n",
    "queries_tfidf = get_TFIDF(qidxs, idfvec)\n",
    "print(queries_tfidf.shape)\n",
    "comments_tfidf = get_TFIDF(cidxs, idfvec)\n",
    "print(comments_tfidf.shape)\n",
    "with open(OUTPUT_VECTORS+\"vectors_tfidf.pkl\", \"wb\") as f:\n",
    "    np.save(f,(queries_tfidf, comments_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec\n",
    "agg=\"sum\"\n",
    "E, _ = embeddings.read_embeddings(OUTPUT_PKL+\"word2vec.txt\", vocab)\n",
    "queries_boe = get_BOE(qidxs, E, agg)\n",
    "print(queries_boe.shape)\n",
    "comments_boe = get_BOE(cidxs, E, agg)\n",
    "print(comments_boe.shape)\n",
    "with open(OUTPUT_VECTORS+\"vectors_boe.pkl\", \"wb\") as f:\n",
    "    np.save(f,(queries_boe, comments_boe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec tuned\n",
    "agg=\"sum\"\n",
    "Et, _ = embeddings.read_embeddings(OUTPUT_PKL+\"word2vec_tuned.txt\", vocab)\n",
    "queries_boe = get_BOE(qidxs, Et, agg)\n",
    "print(queries_boe.shape)\n",
    "comments_boe = get_BOE(cidxs, Et, agg)\n",
    "print(comments_boe.shape)\n",
    "with open(OUTPUT_VECTORS+\"vectors_boe_tuned.pkl\", \"wb\") as f:\n",
    "    np.save(f,(queries_boe, comments_boe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#topics\n",
    "with open(OUTPUT_PKL+\"/lda.pkl\",\"rb\") as f:\n",
    "    topic_model, _ = pickle.load(f)\n",
    "queries_lda = get_topics(qidxs, vocab, topic_model)\n",
    "print(queries_lda.shape)\n",
    "comments_lda = get_topics(cidxs, vocab, topic_model)\n",
    "print(comments_lda.shape)\n",
    "with open(output_pkl+\"vectors_lda.pkl\", \"wb\") as f:\n",
    "    np.save(f,(queries_lda, comments_lda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_rank(q, D):\n",
    "    simz = np.dot(D,q)/(np.linalg.norm(D)*np.linalg.norm(q))\n",
    "    rank = np.argsort(simz)[::-1]\n",
    "    ranked_simz = simz[rank]\n",
    "    return rank, ranked_simz\n",
    "\n",
    "def similarity_ranks(Q, D, queries, comments, top_k = 5):\n",
    "    results = []\n",
    "    for i in range(Q.shape[0]):\n",
    "        qid = queries.iloc[i][\"requestID\"]\n",
    "        r,s = similarity_rank(Q[i], D)\n",
    "        sentence_ids = comments.iloc[r[:top_k]][\"sentenceID\"].values.tolist()\n",
    "        sims = [str(x) for x in s[:top_k].round(5).tolist()]\n",
    "        results.append([qid]+sentence_ids+sims)\n",
    "\n",
    "    return results\n",
    "\n",
    "df_queries = pd.read_csv(OUTPUT_TXT+\"/queries.csv\")\n",
    "df_comments = pd.read_csv(OUTPUT_TXT+\"/comments.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF\n",
    "with open(OUTPUT_VECTORS+\"vectors_tfidf.pkl\", \"rb\") as f:\n",
    "    queries_tfidf, comments_tfidf = np.load(f)\n",
    "results = similarity_ranks(queries_tfidf, comments_tfidf, df_queries, df_comments)\n",
    "with open(OUTPUT_VECTORS+\"rank_tfidf.csv\",\"w\") as fo:\n",
    "    for r in results:\n",
    "        fo.write(\",\".join(r)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BOE\n",
    "with open(OUTPUT_VECTORS+\"vectors_boe.pkl\", \"rb\") as f:\n",
    "    queries_boe, comments_boe = np.load(f)\n",
    "results = similarity_ranks(queries_boe, comments_boe, df_queries, df_comments)\n",
    "with open(OUTPUT_VECTORS+\"rank_boe.csv\",\"w\") as fo:\n",
    "    for r in results:\n",
    "        fo.write(\",\".join(r)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BOE tuned\n",
    "with open(OUTPUT_VECTORS+\"vectors_boe_tuned.pkl\", \"rb\") as f:\n",
    "    queries_boet, comments_boet = np.load(f)\n",
    "results = similarity_ranks(queries_boet, comments_boet, df_queries, df_comments)\n",
    "with open(OUTPUT_VECTORS+\"rank_boe_tuned.csv\",\"w\") as fo:\n",
    "    for r in results:\n",
    "        fo.write(\",\".join(r)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
