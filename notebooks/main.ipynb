{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import csv\n",
    "import lda\n",
    "from math import log\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pdb import set_trace\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import scipy as sp\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "\n",
    "#add ASMAT toolkit\n",
    "ASMAT_PATH=\"/Users/samir/Dev/projects/ASMAT2\"\n",
    "sys.path.append(ASMAT_PATH)\n",
    "sys.path.append(\"..\")\n",
    "from ASMAT import vectorizer, embeddings, features\n",
    "from ASMAT.toolkit import gensimer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths\n",
    "HOME=\"/Users/samir/Dev/projects/feedback_request_aligner/fra/\"\n",
    "FEEDBACK_REQUESTS_PATH = HOME+\"DATA/raw/regulations_proposed_rules_feedback.csv\"\n",
    "QUERIES_PATH = HOME+\"DATA/raw/regulations_proposed_rules_feedback_queries.tsv\"\n",
    "\n",
    "CIGARRETES_COMMENTS_PATH=HOME+\"DATA/raw/all_data_cigarettes\"\n",
    "TOBACCO_COMMENTS_PATH=HOME+\"DATA/raw/all_data_tobacco\"\n",
    "\n",
    "\n",
    "\n",
    "WORD2VEC_INPUT=HOME+\"DATA/embeddings/skip_50.txt\"\n",
    "WORD2VEC_INPUT=HOME+\"DATA/embeddings/glove.42B.300d.txt\"\n",
    "\n",
    "\n",
    "OUTPUT_TXT = HOME+\"DATA/processed/txt/\"\n",
    "OUTPUT_PKL = HOME+\"DATA/processed/pkl/\"\n",
    "OUTPUT_VECTORS = HOME+\"DATA/processed/vectors/\"\n",
    "OUTPUT_RANKINGS = HOME+\"DATA/processed/rankings/\"\n",
    "\n",
    "WORD2VEC=OUTPUT_PKL+\"/word2vec.txt\"\n",
    "WORD2VEC_TUNED=OUTPUT_PKL+\"/word2vec_tuned.txt\"\n",
    "COMMENTS_PATH=OUTPUT_TXT+\"/all_comments.txt\"\n",
    "CORPUS=OUTPUT_TXT+\"all_text.txt\"\n",
    "FILTERED_COMMENTS=OUTPUT_TXT+\"/filtered_comments.txt\"\n",
    "QUERIES=OUTPUT_TXT+\"/queries.txt\"\n",
    "VOCABULARY_PATH=OUTPUT_PKL+\"vocabulary.pkl\"\n",
    "IDF_ESTIMATE_PATH=OUTPUT_PKL+\"IDF.pkl\"\n",
    "\n",
    "\n",
    "if not os.path.exists(OUTPUT_TXT):\n",
    "    os.makedirs(OUTPUT_TXT)\n",
    "if not os.path.exists(OUTPUT_PKL):\n",
    "    os.makedirs(OUTPUT_PKL)\n",
    "if not os.path.exists(OUTPUT_VECTORS):\n",
    "    os.makedirs(OUTPUT_VECTORS)\n",
    "if not os.path.exists(OUTPUT_RANKINGS):\n",
    "    os.makedirs(OUTPUT_RANKINGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read RGOV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "MIN_Q_LEN = 100\n",
    "\n",
    "stop_wordz = set(stopwords.words('english'))\n",
    "remove_punctuation = str.maketrans('', '', string.punctuation+\"”“\")\n",
    "QUOTES_REGEX=r'[\\\"“](.+?)[\\\"”]'\n",
    "\n",
    "\n",
    "def read_raw_rgov(path):\n",
    "    df = pd.DataFrame([])\n",
    "    for f in os.listdir(path):\n",
    "        fname=os.path.join(path,f)\n",
    "        with open(fname,\"r\") as jf:\n",
    "            try:\n",
    "                raw_data = json.load(jf)\n",
    "            except UnicodeDecodeError:\n",
    "                print(\"\\n\\nCould not read file {}\\n\\n\".format(fname))\n",
    "                continue\n",
    "            try:\n",
    "                df = df.append(pd.DataFrame(raw_data[\"documents\"]))\n",
    "            except KeyError:\n",
    "                print(\"\\n\\nCould not find any documents in file {}\\n\\n\".format(fname))\n",
    "                continue\n",
    "#             print(fname)\n",
    "\n",
    "    return df\n",
    "\n",
    "def count_dockets(df):\n",
    "    aggz=df.groupby(\"docketId\").size()\n",
    "    target_dockets = ['FDA-2014-N-0189', 'FDA-2017-N-6565', 'FDA-2017-N-6189', 'FDA-2017-N-6107', 'FDA-2013-N-0521', 'FDA-2015-N-1514', 'FDA-2012-N-1148', 'FDA-2011-N-0467', 'FDA-2017-N-6529', 'FDA-2011-N-0493', 'FDA-2017-N-5095']\n",
    "    for docket in target_dockets:\n",
    "        try:\n",
    "            print(\"{} {}\".format(docket,aggz[docket]))\n",
    "        except KeyError:\n",
    "            print(\"{} {}\".format(docket,\"NULL\"))\n",
    "\n",
    "            \n",
    "def mask_quotes(text):      \n",
    "    matches=re.sub(QUOTES_REGEX,\"00quote00\",text)\n",
    "    return matches\n",
    "\n",
    "def preprocess(d):\n",
    "    d = d.lower()\n",
    "    d = d.replace(\"\\n\", \"\\t\")\n",
    "    #remove stop words and punctuation\n",
    "    d = \" \".join([w.translate(remove_punctuation) for w in d.split() if w not in stop_wordz])\n",
    "    return d\n",
    "     \n",
    "def process_comments(df):\n",
    "    #filter for comments\n",
    "    comments = []\n",
    "    doc_counters=defaultdict(int)\n",
    "    for _, comment in df.iterrows():    \n",
    "        #segment comment into sentences            \n",
    "        txt = comment[\"commentText\"]     \n",
    "        doc_counters[comment[\"docketId\"]]+=1\n",
    "        #document id: docketID#C(id)\n",
    "        docid = comment[\"docketId\"]+\"#C\"+str(doc_counters[comment[\"docketId\"]])\n",
    "        try:\n",
    "            sentences = sent_tokenize(txt)\n",
    "        except TypeError:\n",
    "            print(\"failed tokenizer\")\n",
    "            continue\n",
    "        c = [[ comment[\"docketId\"], docid, \\\n",
    "            docid+\"#S\"+str(i), s, preprocess(mask_quotes(s)), len(s.split())] \\\n",
    "            for i,s in enumerate(sentences)]\n",
    "        comments += c\n",
    "    df = pd.DataFrame(comments,columns=[\"docketId\", \"commentId\", \"sentenceId\",\"text\", \"clean_text\", \"len\"])\n",
    "    return df\n",
    "\n",
    "def process_queries(path):\n",
    "    queries = defaultdict(list)\n",
    "    with open(path) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter='\\t')    \n",
    "        next(csv_reader)\n",
    "        for row in csv_reader:\n",
    "            docket_id, query = row\n",
    "            queries[docket_id].append(query)\n",
    "    ordered_queries = []\n",
    "    for did, qs in queries.items():\n",
    "        for i, q in enumerate(qs):\n",
    "            x = [did,did+\"#Q\"+str(i),q]\n",
    "            ordered_queries.append(x)\n",
    "    df = pd.DataFrame(ordered_queries, columns=[\"docketId\",\"queryId\",\"text\"])\n",
    "    #preprocess text\n",
    "    df[\"clean_text\"] = df[\"text\"].map(preprocess)\n",
    "    return df\n",
    "\n",
    "def extract_comments():\n",
    "    #read documents\n",
    "    df_tob = read_raw_rgov(TOBACCO_COMMENTS_PATH)\n",
    "    df_cig= read_raw_rgov(CIGARRETES_COMMENTS_PATH)\n",
    "    df_all = df_tob.append(df_cig)\n",
    "    #filter comments\n",
    "    df_all = df_all[df_all[\"documentType\"] == \"Public Submission\"] \n",
    "    #remove empty comments\n",
    "    df_all = df_all.dropna(subset=['commentText'])\n",
    "    #remove entries with attachments\n",
    "    print(\"no attachments\")\n",
    "    df_all = df_all[df_all[\"attachmentCount\"] == 0] \n",
    "    #process comments\n",
    "    df_comments = process_comments(df_all)\n",
    "    df_comments.to_csv(COMMENTS_PATH, header=True, index=False)\n",
    "    return df_comments\n",
    "\n",
    "# X = extract_comments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Background Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def generate_background_corpus():\n",
    "    #read comments\n",
    "    df_comments = pd.read_csv(COMMENTS_PATH)\n",
    "    #read feedback requests\n",
    "    df = pd.read_csv(FEEDBACK_REQUESTS_PATH)\n",
    "\n",
    "    #extract all the text \n",
    "    requests_text = df[\"docket_title\"].values.tolist() + df[\"summary\"].values.tolist() + df[\"feedback_asked\"].values.tolist()\n",
    "    #preprocess text\n",
    "    clean_requests_text = [preprocess(str(w)) for w in requests_text]\n",
    "    clean_comments_text = [str(d) for d in df_comments[\"clean_text\"].values.tolist()]\n",
    "    all_text = clean_requests_text + clean_comments_text\n",
    "    #shuffle text \n",
    "    random.shuffle(all_text)\n",
    "    with open(CORPUS,\"w\") as f:\n",
    "        f.write(\"\\n\".join(all_text))\n",
    "\n",
    "# generate_background_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Prepare Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def prepare_queries():\n",
    "    #read queries\n",
    "    df_queries =  process_queries(QUERIES_PATH)\n",
    "    #extract target dockets\n",
    "    dockets = df_queries.docketId.unique().tolist()\n",
    "    target_dockets = []\n",
    "    #read comments\n",
    "    df_comments = pd.read_csv(COMMENTS_PATH)\n",
    "    print(\"all comments: {}\".format(str(len(df_comments))))    \n",
    "    df_comments = df_comments.drop_duplicates(subset=[\"docketId\",\"clean_text\"])\n",
    "    print(\"no duplicates: {}\".format(str(len(df_comments))))\n",
    "    df_comments = df_comments[df_comments[\"len\"] > 5]\n",
    "    print(\"no shorties: {}\".format(str(len(df_comments))))\n",
    "    for docket in dockets:\n",
    "        queries = df_queries[df_queries[\"docketId\"] == docket]\n",
    "        comments = df_comments[df_comments[\"docketId\"] == docket]\n",
    "        if len(comments)>0:\n",
    "            print(docket + \" \" + str(len(comments)))\n",
    "            target_dockets.append(docket)\n",
    "            #save queries and commments\n",
    "            queries.to_csv(OUTPUT_TXT+\"{}_queries.csv\".format(docket), header=True, index=False)\n",
    "            comments.to_csv(OUTPUT_TXT+\"{}_comments.csv\".format(docket), header=True, index=False)\n",
    "#target dockets = ['FDA-2014-N-0189', 'FDA-2017-N-6565', 'FDA-2017-N-6189', 'FDA-2017-N-6107', 'FDA-2013-N-0521', 'FDA-2015-N-1514', 'FDA-2012-N-1148', 'FDA-2011-N-0467', 'FDA-2017-N-6529', 'FDA-2011-N-0493', 'FDA-2017-N-5095']\n",
    "# prepare_queries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inverse document frequency\n",
    "def getIDF(N, t):\n",
    "    return log(float(N)/float(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_WORD_FREQ=5\n",
    "def compute_vocabulary():\n",
    "    with open(CORPUS,\"r\") as f:\n",
    "        all_text =   f.readlines()\n",
    "    #get vocabulary\n",
    "    vocab = vectorizer.build_vocabulary(all_text, min_freq=MIN_WORD_FREQ)\n",
    "    print(\"vocabulary size: {}\".format(len(vocab)))\n",
    "    #save vocabulary\n",
    "    with open(VOCABULARY_PATH,\"wb\") as f:\n",
    "        pickle.dump(vocab,f)\n",
    "# compute_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_IDF():\n",
    "    with open(VOCABULARY_PATH,\"rb\") as f:\n",
    "        vocab = pickle.load(f)\n",
    "    with open(CORPUS,\"r\") as f:\n",
    "        all_text =   f.readlines()\n",
    "    #compute document frequencies\n",
    "    all_idxs, _ = vectorizer.docs2idx(all_text, vocab)\n",
    "    ndocs = len(all_idxs)\n",
    "    docfreq = Counter(str(x) for xs in all_idxs for x in set(xs))\n",
    "    #inverse document frequencies\n",
    "    idfs = {w: getIDF(ndocs, docfreq[w]) for w in docfreq}\n",
    "    #get an IDF vector \n",
    "    idfvec = np.zeros(len(idfs))\n",
    "    for w, v in idfs.items(): idfvec[int(w)] = v\n",
    "    with open(OUTPUT_PKL+\"/IDF.pkl\",\"wb\") as f:\n",
    "        pickle.dump(idfvec,f)\n",
    "\n",
    "# compute_IDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_DIM=300\n",
    "NEGATIVE_SAMPLES=10\n",
    "EPOCHS=5\n",
    "#extract word embeddings\n",
    "def get_word_embeddings():\n",
    "    with open(VOCABULARY_PATH,\"rb\") as f:\n",
    "        vocab = pickle.load(f)\n",
    "    embeddings.extract_embeddings(WORD2VEC_INPUT, WORD2VEC, vocab)\n",
    "\n",
    "def update_word_embeddings():\n",
    "    #update word embeddings \n",
    "    train_seq = gensimer.Word2VecReader([CORPUS])\n",
    "    w2v = gensimer.get_skipgram(dim=VECTOR_DIM,negative_samples=NEGATIVE_SAMPLES, min_freq=MIN_WORD_FREQ)\n",
    "    w2v_trained = gensimer.train_skipgram(w2v, train_seq, epochs=EPOCHS,\n",
    "                                          path_out=WORD2VEC_TUNED,\n",
    "                                          pretrained_weights_path=WORD2VEC)\n",
    "\n",
    "# get_word_embeddings()\n",
    "# update_word_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train topic model\n",
    "def train_topic_model():\n",
    "    with open(VOCABULARY_PATH,\"rb\") as f:\n",
    "        vocab = pickle.load(f)\n",
    "    with open(CORPUS,\"r\") as f:\n",
    "        all_text =   f.readlines()\n",
    "    all_idxs, _ = vectorizer.docs2idx(all_text, vocab)\n",
    "    n_topics=100\n",
    "    n_iter=100\n",
    "    X = features.BOW_freq(all_idxs, len(vocab), sparse=True)\n",
    "    X = X.astype('int32')\n",
    "    topic_model = lda.LDA(n_topics=n_topics, n_iter=n_iter)\n",
    "    topic_model.fit(X)\n",
    "    #save model\n",
    "    with open(OUTPUT_PKL+\"/lda.pkl\",\"wb\") as f:\n",
    "        pickle.dump([topic_model, vocab], f)\n",
    "\n",
    "# train_topic_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BERT_MAX_INPUT=510\n",
    "BERT_BATCH_SIZE = 100\n",
    "\n",
    "def TFIDF_vectors(docket, vocab, idfs):\n",
    "    queries, comments = vectorize_docket(docket, vocab)\n",
    "    Q = features.BOW_freq(queries, len(vocab), sparse=True)\n",
    "    C = features.BOW_freq(comments, len(vocab), sparse=True)\n",
    "#     Q*=idfs\n",
    "#     C*=idfs\n",
    "    \n",
    "    return Q.tocsc(), C.tocsc()\n",
    "\n",
    "def BOE_vectors(docket, vocab, E, agg):\n",
    "    queries, comments = vectorize_docket(docket, vocab)\n",
    "    Q = features.BOE(queries, E, agg)\n",
    "    C = features.BOE(comments, E, agg)\n",
    "    return Q,C\n",
    "\n",
    "def LDA_vectors(docket, vocab, topic_model):\n",
    "    queries, comments = vectorize_docket(docket, vocab)\n",
    "    Q = features.BOW_freq(queries, len(vocab), sparse=True)\n",
    "    C = features.BOW_freq(comments, len(vocab), sparse=True)\n",
    "    Q = Q.astype('int32')\n",
    "    C = C.astype('int32')\n",
    "    Qt = topic_model.transform(Q)\n",
    "    Ct = topic_model.transform(C)\n",
    "    return Qt, Ct\n",
    "\n",
    "def transformer_encoder(tokenizer, encoder, D):    \n",
    "    tokens_tensors = []\n",
    "    segments_tensors = []\n",
    "    tokenized_texts = []\n",
    "    \n",
    "    bertify = \"[CLS] {} [SEP]\"  \n",
    "    tokenized_texts = [tokenizer.tokenize(bertify.format(doc)) for doc in D] \n",
    "\n",
    "    #count the document lengths  \n",
    "    max_len = max([len(d) for d in tokenized_texts]) \n",
    "    #document cannot exceed BERT input matrix size \n",
    "    max_len = min(BERT_MAX_INPUT, max_len)\n",
    "    # print(\"[max len: {}]\".format(max_len))\n",
    "    for tokens in tokenized_texts:   \n",
    "        # Convert tokens to vocabulary indices\n",
    "        idxs = tokenizer.convert_tokens_to_ids(tokens)        \n",
    "        #truncate sentences longer than what BERT supports\n",
    "        if len(idxs) > BERT_MAX_INPUT: idxs = idxs[:BERT_MAX_INPUT]\n",
    "        pad_size = max_len - len(idxs)\n",
    "        #add padding to indexed tokens\n",
    "        idxs+=[0] * pad_size\n",
    "        segments_ids = [0] * len(idxs) \n",
    "        tokens_tensors.append(torch.tensor([idxs]))\n",
    "        segments_tensors.append(torch.tensor([segments_ids]))\n",
    "    \n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.cat(tokens_tensors)\n",
    "    segments_tensor = torch.cat(segments_tensors)\n",
    "    \n",
    "    #set encoder to eval mode\n",
    "    encoder.eval()\n",
    "    with torch.no_grad():        \n",
    "        pool_features, cls_features = encoder(tokens_tensor, token_type_ids=segments_tensor)    \n",
    "        pool_features = pool_features.sum(axis=1)\n",
    "    return cls_features.numpy(), pool_features.numpy()\n",
    "\n",
    "def BERT_vectors(docket, tokenizer, model):\n",
    "    df_queries = pd.read_csv(OUTPUT_TXT+\"{}_queries.csv\".format(docket))\n",
    "    df_comments = pd.read_csv(OUTPUT_TXT+\"{}_comments.csv\".format(docket))\n",
    "    df_queries = df_queries.dropna(subset=['clean_text'])\n",
    "    df_comments = df_comments.dropna(subset=['clean_text'])\n",
    "    #BERT\n",
    "    query_pool_vectors = []\n",
    "    query_cls_vectors = []\n",
    "    all_queries = df_queries[\"clean_text\"]\n",
    "    n_batches = int(len(all_queries)/BERT_BATCH_SIZE)+1\n",
    "    for j in range(n_batches):\n",
    "        query_batch = all_queries[BERT_BATCH_SIZE*j:BERT_BATCH_SIZE*(j+1)]\n",
    "        if len(query_batch) > 0:\n",
    "            sys.stdout.write(\"\\rquery batch:{}\\{} (size: {})\".format(j+1,n_batches, str(len(query_batch))))\n",
    "            sys.stdout.flush()\n",
    "            Q_cls, Q_pool = transformer_encoder(tokenizer, model, query_batch)\n",
    "            # set_trace()\n",
    "            query_cls_vectors.append(Q_cls)\n",
    "            query_pool_vectors.append(Q_pool)            \n",
    "    query_pool_vectors = np.vstack(query_pool_vectors)\n",
    "    query_cls_vectors = np.vstack(query_cls_vectors)\n",
    "    print()\n",
    "    comment_cls_vectors = []\n",
    "    comment_pool_vectors = []\n",
    "    all_comments = df_comments[\"clean_text\"]\n",
    "    n_batches = int(len(all_comments)/BERT_BATCH_SIZE)+1\n",
    "    for j in range(n_batches):\n",
    "        comment_batch = all_comments[BERT_BATCH_SIZE*j:BERT_BATCH_SIZE*(j+1)]\n",
    "        if len(comment_batch) > 0:\n",
    "            sys.stdout.write(\"\\rcomment batch:{}\\{} (size: {})\".format(j+1,n_batches, str(len(comment_batch))))\n",
    "            sys.stdout.flush()\n",
    "            C_cls, C_pool = transformer_encoder(tokenizer, model, comment_batch)\n",
    "            comment_cls_vectors.append(C_cls)\n",
    "            comment_pool_vectors.append(C_pool)\n",
    "    comment_pool_vectors = np.vstack(comment_pool_vectors)\n",
    "    comment_cls_vectors = np.vstack(comment_cls_vectors)\n",
    "    print()\n",
    "    return query_cls_vectors, comment_cls_vectors, query_pool_vectors, comment_pool_vectors\n",
    "\n",
    "\n",
    "def vectorize_docket(docket, vocab):\n",
    "    df_queries = pd.read_csv(OUTPUT_TXT+\"{}_queries.csv\".format(docket))\n",
    "    df_comments = pd.read_csv(OUTPUT_TXT+\"{}_comments.csv\".format(docket))\n",
    "    df_queries = df_queries.dropna(subset=['clean_text'])\n",
    "    df_comments = df_comments.dropna(subset=['clean_text'])\n",
    "    qidxs, _  = vectorizer.docs2idx(df_queries[\"clean_text\"], vocab)\n",
    "    cidxs, _  = vectorizer.docs2idx(df_comments[\"clean_text\"], vocab)\n",
    "    return qidxs, cidxs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dockets = ['FDA-2014-N-0189', 'FDA-2013-N-0521', \n",
    "                  'FDA-2015-N-1514', 'FDA-2012-N-1148', \n",
    "                  'FDA-2011-N-0467']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_TFIDF(target_dockets):\n",
    "    with open(VOCABULARY_PATH,\"rb\") as f:\n",
    "        vocab = pickle.load(f)\n",
    "    with open(OUTPUT_PKL+\"/IDF.pkl\",\"rb\") as f:\n",
    "        idfvec = pickle.load(f)\n",
    "    print(\"[building TF-IDF vectors]\")\n",
    "    for docket in target_dockets:\n",
    "        print(\"[vectorizing docket: {}]\".format(docket))\n",
    "        Q,C = TFIDF_vectors(docket, vocab, idfvec)\n",
    "        sp.sparse.save_npz(OUTPUT_VECTORS+\"{}_queries_tf-idf\".format(docket), Q)\n",
    "        sp.sparse.save_npz(OUTPUT_VECTORS+\"{}_comments_tf-idf\".format(docket), C)\n",
    "    print(\"[done]\")\n",
    "vectorize_TFIDF(target_dockets[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_BERT(target_dockets):\n",
    "    #BERT\n",
    "    BERT_MODEL = 'bert-base-uncased'\n",
    "    # Load pre-trained model tokenizer (vocabulary)\n",
    "    tokenizer = BertTokenizer.from_pretrained(BERT_MODEL)\n",
    "    # Load pre-trained model (weights)\n",
    "    model = BertModel.from_pretrained(BERT_MODEL, output_hidden_states=False)\n",
    "\n",
    "    print(\"[building BERT vectors]\")\n",
    "    for docket in target_dockets:\n",
    "        print(\"> docket: {}\".format(docket))\n",
    "        Q_cls, C_cls, Q_pool, C_pool = BERT_vectors(docket, tokenizer, model)            \n",
    "        with open(OUTPUT_VECTORS+\"{}_queries_bert_cls.np\".format(docket),\"wb\") as f:\n",
    "            np.save(f, Q_cls)\n",
    "        with open(OUTPUT_VECTORS+\"{}_comments_bert_cls.np\".format(docket),\"wb\") as f:\n",
    "            np.save(f, C_cls)\n",
    "        with open(OUTPUT_VECTORS+\"{}_queries_bert_pool.np\".format(docket),\"wb\") as f:\n",
    "            np.save(f, Q_pool)\n",
    "        with open(OUTPUT_VECTORS+\"{}_comments_bert_pool.np\".format(docket),\"wb\") as f:\n",
    "            np.save(f,  C_pool)\n",
    "    print(\"[done]\")\n",
    "\n",
    "# vectorize_BERT(target_dockets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec\n",
    "def vectorize_BOE(target_dockets):\n",
    "    with open(VOCABULARY_PATH,\"rb\") as f:\n",
    "        vocab = pickle.load(f)\n",
    "    agg=\"sum\"\n",
    "    E, _ = embeddings.read_embeddings(WORD2VEC, vocab)\n",
    "    \n",
    "    print(\"[building BOE vectors]\")\n",
    "    for docket in target_dockets:\n",
    "        print(\"[vectorizing docket: {}]\".format(docket))\n",
    "        Q,C = BOE_vectors(docket, vocab, E, agg)    \n",
    "        with open(OUTPUT_VECTORS+\"{}_queries_boe.np\".format(docket),\"wb\") as f:\n",
    "            np.save(f, Q)\n",
    "        with open(OUTPUT_VECTORS+\"{}_comments_boe.np\".format(docket),\"wb\") as f:\n",
    "            np.save(f, C)\n",
    "    print(\"[done]\")\n",
    "    \n",
    "# vectorize_BOE(target_dockets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_BOE_tuned(target_dockets):\n",
    "    agg=\"sum\"\n",
    "    with open(VOCABULARY_PATH,\"rb\") as f:\n",
    "        vocab = pickle.load(f)\n",
    "    E_tuned, _ = embeddings.read_embeddings(WORD2VEC_TUNED, vocab)\n",
    "    \n",
    "    print(\"[building fine-tuned BOE vectors]\")\n",
    "    for docket in target_dockets:\n",
    "        print(\"[vectorizing docket: {}]\".format(docket))\n",
    "        Q,C = BOE_vectors(docket, vocab, E_tuned, agg)\n",
    "        with open(OUTPUT_VECTORS+\"{}_queries_boe_tuned.np\".format(docket),\"wb\") as f:\n",
    "            np.save(f, Q)\n",
    "        with open(OUTPUT_VECTORS+\"{}_comments_boe_tuned.np\".format(docket),\"wb\") as f:\n",
    "            np.save(f, C)\n",
    "    print(\"[done]\")\n",
    "\n",
    "# vectorize_BOE_tuned(target_dockets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_LDA(target_dockets):\n",
    "    with open(VOCABULARY_PATH,\"rb\") as f:\n",
    "        vocab = pickle.load(f)\n",
    "    #topics\n",
    "    with open(OUTPUT_PKL+\"/lda.pkl\",\"rb\") as f:\n",
    "        topic_model, _ = pickle.load(f)\n",
    "    print(\"[building LDA vectors]\")\n",
    "    for docket in target_dockets:\n",
    "        print(\"[vectorizing docket: {}]\".format(docket))\n",
    "        Q,C = LDA_vectors(docket, vocab, topic_model)\n",
    "    with open(OUTPUT_VECTORS+\"{}_queries_lda.np\".format(docket),\"wb\") as f:\n",
    "            np.save(f, Q)\n",
    "    with open(OUTPUT_VECTORS+\"{}_comments_lda.np\".format(docket),\"wb\") as f:\n",
    "            np.save(f, C)\n",
    "    print(\"[done]\")\n",
    "# vectorize_LDA(target_dockets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def similarity_ranks(Q, C, queries, comments, method, top_k):\n",
    "    assert len(queries) == Q.shape[0]\n",
    "    assert len(comments) == C.shape[0]\n",
    "    S = cosine_similarity(Q,C)\n",
    "    ranks = np.argsort(S, axis=1)[:,::-1]\n",
    "    top_ranks = ranks[:,:top_k]\n",
    "    results = []\n",
    "    for i in range(top_ranks.shape[0]):\n",
    "        rank_i  = top_ranks[i]\n",
    "        sims = S[i, rank_i]  \n",
    "        sims = [str(x.round(3)) for x in sims]\n",
    "        sentence_ids = comments[\"sentenceId\"].iloc[rank_i].values.tolist()   \n",
    "        sentences = comments[\"text\"].iloc[rank_i].values.tolist()\n",
    "        query_id = queries.iloc[i][\"queryId\"]\n",
    "        results+=[[method,query_id,sid,sim,txt.replace(\"\\n\",\" \").replace(\"\\t\",\" \")] \n",
    "                  for sid,sim,txt in zip(sentence_ids, sims, sentences)]\n",
    "    return results\n",
    "\n",
    "\n",
    "def rank_docket(docket, methods, top_k=5):\n",
    "    docket_data = {}\n",
    "    #load docket data\n",
    "    all_results = []\n",
    "    print(\"[reading docket: {}]\".format(docket))\n",
    "    df_queries = pd.read_csv(OUTPUT_TXT+\"{}_queries.csv\".format(docket))\n",
    "    df_comments = pd.read_csv(OUTPUT_TXT+\"{}_comments.csv\".format(docket))\n",
    "    df_queries = df_queries.dropna(subset=['clean_text'])\n",
    "    df_comments = df_comments.dropna(subset=['clean_text'])\n",
    "      \n",
    "    #ranks for each method\n",
    "    ranks = []\n",
    "    for m in methods:\n",
    "        print(\"[ranking method: {}]\".format(m))\n",
    "        if m == \"tf-idf\":\n",
    "            Q = sp.sparse.load_npz(OUTPUT_VECTORS+\"{}_queries_tf-idf.npz\".format(docket))\n",
    "            C = sp.sparse.load_npz(OUTPUT_VECTORS+\"{}_comments_tf-idf.npz\".format(docket))\n",
    "        else:\n",
    "            with open(OUTPUT_VECTORS+\"{}_queries_{}.np\".format(docket,m),\"rb\") as f:\n",
    "                Q = np.load(f)\n",
    "            with open(OUTPUT_VECTORS+\"{}_comments_{}.np\".format(docket,m),\"rb\") as f:\n",
    "                C = np.load(f)      \n",
    "        rank = similarity_ranks(Q, C, df_queries, df_comments, m, top_k)\n",
    "        ranks+=rank\n",
    "    \n",
    "    with open(OUTPUT_RANKINGS+\"{}_rank.csv\".format(docket),\"w\") as fo:\n",
    "        top_sentences = []\n",
    "        for r in ranks:\n",
    "            fo.write(\"\\t\".join([docket]+r)+\"\\n\")\n",
    "    return ranks    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# docket=\"FDA-2011-N-0467\"\n",
    "# docket='FDA-2014-N-0189'\n",
    "# X = rank_docket(docket,[\"boe\",\"boe_tuned\",\"tf-idf\"])\n",
    "# X = rank_docket(docket,[\"boe\",\"tf-idf\"])\n",
    "target_dockets = ['FDA-2014-N-0189', 'FDA-2013-N-0521', \n",
    "                  'FDA-2015-N-1514', 'FDA-2012-N-1148',\n",
    "                  'FDA-2011-N-0467']\n",
    "\n",
    "for docket in target_dockets:\n",
    "    ranks = rank_docket(docket,[\"boe\",\"boe_tuned\",\"tf-idf\",\"bert_cls\",\"bert_pool\",\"lda\"])\n",
    "#     all_ranks+=rank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat /Users/samir/Dev/projects/feedback_request_aligner/fra/DATA/processed/rankings/* > /Users/samir/Dev/projects/feedback_request_aligner/fra/DATA/processed/rankings/all_rankings.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dockets = ['FDA-2014-N-0189', 'FDA-2013-N-0521', \n",
    "                  'FDA-2015-N-1514', 'FDA-2012-N-1148',\n",
    "                  'FDA-2011-N-0467']\n",
    "for docket in target_dockets:\n",
    "    try:\n",
    "        rank_docket(docket,[\"lda\"])\n",
    "    except FileNotFoundError:\n",
    "        print(\"not found {}\".format(docket))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data\n",
    "print(\"Processing Data\")\n",
    "extract_comments()\n",
    "generate_background_corpus()\n",
    "prepare_queries()\n",
    "#compute embeddings\n",
    "print(\"computing embeddings\")\n",
    "compute_vocabulary()\n",
    "compute_IDF()\n",
    "get_word_embeddings()\n",
    "update_word_embeddings()\n",
    "train_topic_model()\n",
    "#vectorize\n",
    "print(\"vectorizing\")\n",
    "target_dockets = ['FDA-2014-N-0189', 'FDA-2013-N-0521', \n",
    "                  'FDA-2015-N-1514', 'FDA-2012-N-1148', \n",
    "                  'FDA-2011-N-0467'][::-1]\n",
    "vectorize_BERT(target_dockets)\n",
    "vectorize_LDA(target_dockets)\n",
    "vectorize_BOE(target_dockets)\n",
    "vectorize_TFIDF(target_dockets)\n",
    "vectorize_BOE_tuned(target_dockets)\n",
    "#rank all dockets\n",
    "target_dockets = ['FDA-2014-N-0189', 'FDA-2013-N-0521', \n",
    "                  'FDA-2015-N-1514', 'FDA-2012-N-1148',\n",
    "                  'FDA-2011-N-0467']\n",
    "for docket in target_dockets[:\n",
    "    rank_docket(docket,[\"boe\",\"boe_tuned\",\"tf-idf\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
